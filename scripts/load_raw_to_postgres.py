import os
import json
import glob
from datetime import datetime
from typing import Iterator, Dict, Any, List

import psycopg2
from psycopg2.extras import execute_values


POSTGRES_DSN = os.getenv(
    "DATABASE_URL",
    "postgresql://user:password@localhost:5432/medical_warehouse",
)

DATA_LAKE_BASE = os.path.join(
    os.path.dirname(__file__), "..", "data", "raw", "telegram_messages"
)

RAW_SCHEMA = os.getenv("RAW_SCHEMA", "raw")
RAW_TABLE = "telegram_messages"


def iter_json_messages(base_dir: str) -> Iterator[Dict[str, Any]]:
    # Expect structure: data/raw/telegram_messages/YYYY-MM-DD/*.json
    base_dir = os.path.abspath(base_dir)
    for date_dir in sorted(glob.glob(os.path.join(base_dir, "*"))):
        if not os.path.isdir(date_dir):
            continue
        for fp in glob.glob(os.path.join(date_dir, "*.json")):
            with open(fp, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                except json.JSONDecodeError:
                    continue
                if isinstance(data, list):
                    for m in data:
                        if isinstance(m, dict):
                            yield m
                elif isinstance(data, dict):
                    # Could be NDJSON-like or dict with 'messages'
                    messages = data.get("messages") if isinstance(data, dict) else None
                    if isinstance(messages, list):
                        for m in messages:
                            if isinstance(m, dict):
                                yield m


def ensure_raw_table(conn):
    with conn.cursor() as cur:
        cur.execute(f"CREATE SCHEMA IF NOT EXISTS {RAW_SCHEMA}")
        # Create a wide raw table with common Telegram fields; use JSONB for payload
        cur.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {RAW_SCHEMA}.{RAW_TABLE} (
                id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
                message_id BIGINT,
                channel_id BIGINT,
                channel_username TEXT,
                channel_name TEXT,
                message_text TEXT,
                message_date TIMESTAMP WITH TIME ZONE,
                view_count BIGINT,
                forward_count BIGINT,
                has_image BOOLEAN,
                raw_payload JSONB,
                load_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            )
            """
        )
    conn.commit()


def coerce_record(m: Dict[str, Any]) -> Dict[str, Any]:
    def get_first(*keys, default=None):
        for k in keys:
            if k in m and m[k] is not None:
                return m[k]
        return default

    # Parse date
    dt_raw = get_first("date", "message_date")
    dt = None
    if isinstance(dt_raw, str):
        for fmt in ("%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%d %H:%M:%S%z", "%Y-%m-%d %H:%M:%S", "%Y-%m-%dT%H:%M:%S"):
            try:
                dt = datetime.strptime(dt_raw, fmt)
                break
            except Exception:
                continue

    has_image = bool(get_first("has_image", "has_media", "photo", default=False))

    return {
        "message_id": get_first("id", "message_id"),
        "channel_id": get_first("chat_id", "channel_id"),
        "channel_username": get_first("chat_username", "channel_username", "channel"),
        "channel_name": get_first("chat_title", "channel_name"),
        "message_text": get_first("message", "text", "message_text"),
        "message_date": dt,
        "view_count": get_first("views", "view_count"),
        "forward_count": get_first("forwards", "forward_count"),
        "has_image": has_image,
        "raw_payload": json.dumps(m, ensure_ascii=False),
    }


def batch_insert(conn, rows: List[Dict[str, Any]]):
    if not rows:
        return
    cols = [
        "message_id",
        "channel_id",
        "channel_username",
        "channel_name",
        "message_text",
        "message_date",
        "view_count",
        "forward_count",
        "has_image",
        "raw_payload",
    ]
    values = [[r.get(c) for c in cols] for r in rows]
    with conn.cursor() as cur:
        execute_values(
            cur,
            f"INSERT INTO {RAW_SCHEMA}.{RAW_TABLE} (" + ",".join(cols) + ") VALUES %s",
            values,
        )
    conn.commit()


def main():
    conn = psycopg2.connect(POSTGRES_DSN)
    try:
        ensure_raw_table(conn)
        buffer: List[Dict[str, Any]] = []
        for m in iter_json_messages(DATA_LAKE_BASE):
            buffer.append(coerce_record(m))
            if len(buffer) >= 1000:
                batch_insert(conn, buffer)
                buffer.clear()
        if buffer:
            batch_insert(conn, buffer)
        print("Load complete")
    finally:
        conn.close()


if __name__ == "__main__":
    main()
